{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a895f0a-1265-48ef-9c02-7c3f3cb6e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "from model import MiniGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5571a313-5e54-43ed-b120-e47baa310db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"C:/Users/aksha/OneDrive/Desktop/llm/tokenizer/tokenizer.model\")\n",
    "\n",
    "def encode(text):\n",
    "    return sp.encode(text)\n",
    "\n",
    "def decode(tokens):\n",
    "    return sp.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e6a8618-6e1e-4ecc-ab31-71e663b0f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "VOCAB_SIZE = 8000\n",
    "BLOCK_SIZE = 128   # ‚Üê THIS WAS MISSING\n",
    "\n",
    "model = MiniGPT().to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(\n",
    "    \"models/minigpt_fin.pt\", \n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8e1618a-4d27-481e-b583-72cbcd80cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model keys: dict_keys(['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded model keys:\", ckpt.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2566c662-4038-4035-a61a-a31e8cf2c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checksum: 3223.0185039695352\n"
     ]
    }
   ],
   "source": [
    "print(\"Model checksum:\", sum(p.sum().item() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9a50b03-a211-4c8d-a33c-0f448cf217a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, top_k=None, repetition_penalty=1.0):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -128:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        for token_id in set(idx[0].tolist()):\n",
    "            logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_idx = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a256b1d-49ab-416d-a62c-aa016053da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_once(user_input, history):\n",
    "    system_prompt = (\n",
    "        \"You are a sarcastic AI.\\n\"\n",
    "        \"You never give direct answers.\\n\"\n",
    "        \"You reply with dry sarcasm, irony, and confident nonsense.\\n\"\n",
    "        \"You do not explain your reasoning.\\n\"\n",
    "        \"You sound amused and slightly judgmental.\\n\"\n",
    "        \"Short to medium replies.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = system_prompt\n",
    "    for u, a in history[-3:]: \n",
    "        prompt += f\"User: {u}\\nAssistant: {a}\\n\"\n",
    "\n",
    "    prompt += f\"User: {user_input}\\nAssistant:\"\n",
    "\n",
    "    idx = torch.tensor(\n",
    "        encode(prompt),\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = generate(\n",
    "            model,\n",
    "            idx,\n",
    "            max_new_tokens=40,\n",
    "            temperature=0.9,\n",
    "            top_k=40\n",
    "        )\n",
    "\n",
    "    text = decode(out[0].tolist())\n",
    "    reply = text[len(prompt):]\n",
    "\n",
    "    if \"User:\" in reply:\n",
    "        reply = reply.split(\"User:\")[0]\n",
    "    if \"Assistant:\" in reply:\n",
    "        reply = reply.split(\"Assistant:\")[-1]\n",
    "\n",
    "    reply = reply.strip()\n",
    "\n",
    "    if len(reply) < 3:\n",
    "        reply = \"Fascinating theory. Truly.\"\n",
    "\n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4590f7de-054a-4281-8618-a60acc44f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI online. Type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Royal Consi internationalressiveomp operatedidence extended Tra Indian increase financial gave join Westire put why increase commercial own appear success while 8 nation married use Tom finds den othersailer termged Washingtonely behind \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: ac yearsarkination travelration in Tay of the use. and the film.s of just was Dick the road. and which the the Dadaz of the New Yorkers 's \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "conversation = []\n",
    "\n",
    "print(\"AI online. Type 'exit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user = input(\"You: \").strip()\n",
    "    if user.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    reply = chat_once(user, conversation)\n",
    "    print(\"AI:\", reply, \"\\n\")\n",
    "\n",
    "    conversation.append((user, reply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62989770-e668-4254-998e-fafcb47f332a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
