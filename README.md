# Team 2 – Individual LLM Contribution

This folder contains my individual contribution to Team 2’s
Sarcastic LLM project.

## What this is
A small transformer-based language model trained
from scratch.

## My contributions
- Implemented the transformer model architecture
- Built the training loop using token-level prediction
- Implemented text generation and sampling
- Experimented with sarcasm-biased prompting

## Dataset
- Experiments were conducted using subsets of WikiText / custom text
- Full-scale datasets (OpenWebText) were avoided due to hardware limits

## Notes
- Outputs may be incoherent or sarcastic by design
- The focus is on understanding the LLM pipeline end-to-end and to assess the sarcasm level of the model.
